{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fd55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# No warnings about setting value on copy of slice\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Display up to 60 columns of a dataframe\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "# Matplotlib visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default font size\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "# Internal ipython tool for setting figure size\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Seaborn for visualization\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "# Splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8bb210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X56</th>\n",
       "      <th>X59</th>\n",
       "      <th>X67</th>\n",
       "      <th>X83</th>\n",
       "      <th>X96</th>\n",
       "      <th>X98</th>\n",
       "      <th>X132</th>\n",
       "      <th>X133</th>\n",
       "      <th>X136</th>\n",
       "      <th>X145</th>\n",
       "      <th>X147</th>\n",
       "      <th>X148</th>\n",
       "      <th>X149</th>\n",
       "      <th>X152</th>\n",
       "      <th>X157</th>\n",
       "      <th>X160</th>\n",
       "      <th>X171</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X188</th>\n",
       "      <th>X195</th>\n",
       "      <th>X197</th>\n",
       "      <th>X201</th>\n",
       "      <th>X202</th>\n",
       "      <th>X204</th>\n",
       "      <th>X208</th>\n",
       "      <th>X213</th>\n",
       "      <th>X218</th>\n",
       "      <th>X228</th>\n",
       "      <th>X234</th>\n",
       "      <th>X239</th>\n",
       "      <th>X242</th>\n",
       "      <th>X245</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>1.360</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.5190</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.22400</td>\n",
       "      <td>-0.1800</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.6550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.986</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.0934</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.5570</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>-0.00985</td>\n",
       "      <td>-0.1180</td>\n",
       "      <td>0.240</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.8990</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.805</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.913</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.0912</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>1.360</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.5190</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.22400</td>\n",
       "      <td>-0.1800</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.6550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.986</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.0934</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.644</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.1350</td>\n",
       "      <td>-0.1770</td>\n",
       "      <td>-1.30000</td>\n",
       "      <td>-0.2520</td>\n",
       "      <td>0.229</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>1.190</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.0497</td>\n",
       "      <td>-0.1990</td>\n",
       "      <td>-0.75500</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X56    X59   X67    X83   X96    X98   X132   X133    X136    X145  \\\n",
       "0  0.150 -0.841 -2.33  1.360 -1.78 -0.984 -0.362 -0.198 -0.5190 -0.0174   \n",
       "1  0.491 -0.891 -2.30  1.220 -1.74 -0.497 -0.578  0.268 -0.5570  0.6600   \n",
       "2  0.150 -0.841 -2.33  1.360 -1.78 -0.984 -0.362 -0.198 -0.5190 -0.0174   \n",
       "3 -0.106  0.644 -2.14  0.974 -1.20 -1.380  0.540  0.142 -0.1350 -0.1770   \n",
       "4 -0.116 -0.149 -2.43  1.190 -1.38 -1.220  0.121  0.166 -0.0497 -0.1990   \n",
       "\n",
       "      X147    X148   X149  X152    X157   X160   X171   X176   X177  X188  \\\n",
       "0 -0.22400 -0.1800 -0.445  1.84 -0.6550 -1.620 -0.793  1.170  0.687 -1.88   \n",
       "1 -0.00985 -0.1180  0.240  1.35 -0.8990 -1.350 -0.559  0.708  0.805 -1.73   \n",
       "2 -0.22400 -0.1800 -0.445  1.84 -0.6550 -1.620 -0.793  1.170  0.687 -1.88   \n",
       "3 -1.30000 -0.2520  0.229  1.21  0.6610 -0.834 -1.040  1.370  0.421 -1.32   \n",
       "4 -0.75500  0.0363 -0.270  1.58 -0.0469 -0.989 -1.090  0.956  0.210 -1.40   \n",
       "\n",
       "    X195   X197  X201  X202   X204   X208  X213  X218   X228  X234    X239  \\\n",
       "0  0.694 -0.387 -1.45  1.30 -1.620  0.986  1.58 -3.22 -0.717  2.33 -0.0934   \n",
       "1  0.392 -0.277 -1.20  1.34 -1.500  0.913  1.72 -2.91 -0.646  2.00 -0.0912   \n",
       "2  0.694 -0.387 -1.45  1.30 -1.620  0.986  1.58 -3.22 -0.717  2.33 -0.0934   \n",
       "3 -0.629  0.405 -1.21  1.16 -0.771  0.341  0.76 -3.54 -0.372  2.01  1.5500   \n",
       "4  0.187 -0.172 -1.11  1.31 -1.150  0.964  1.96 -3.19 -0.763  2.41  0.6050   \n",
       "\n",
       "    X242   X245     y  \n",
       "0 -1.420  0.260  0.57  \n",
       "1 -0.784  0.233  0.20  \n",
       "2 -1.420  0.260  0.84  \n",
       "3 -1.560  0.239  0.80  \n",
       "4 -1.440  0.323  0.82  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data into a dataframe \n",
    "data = pd.read_csv('C:/Users/Administrator/Desktop/机器学习/H.csv')\n",
    "\n",
    "# Display top of dataframe\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc37beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y  = np.array(data['y']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d3095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X56</th>\n",
       "      <th>X59</th>\n",
       "      <th>X67</th>\n",
       "      <th>X83</th>\n",
       "      <th>X96</th>\n",
       "      <th>X98</th>\n",
       "      <th>X132</th>\n",
       "      <th>X133</th>\n",
       "      <th>X136</th>\n",
       "      <th>X145</th>\n",
       "      <th>X147</th>\n",
       "      <th>X148</th>\n",
       "      <th>X149</th>\n",
       "      <th>X152</th>\n",
       "      <th>X157</th>\n",
       "      <th>X160</th>\n",
       "      <th>X171</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X188</th>\n",
       "      <th>X195</th>\n",
       "      <th>X197</th>\n",
       "      <th>X201</th>\n",
       "      <th>X202</th>\n",
       "      <th>X204</th>\n",
       "      <th>X208</th>\n",
       "      <th>X213</th>\n",
       "      <th>X218</th>\n",
       "      <th>X228</th>\n",
       "      <th>X234</th>\n",
       "      <th>X239</th>\n",
       "      <th>X242</th>\n",
       "      <th>X245</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>1.360</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.5190</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.22400</td>\n",
       "      <td>-0.1800</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.6550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.986</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.0934</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.5570</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>-0.00985</td>\n",
       "      <td>-0.1180</td>\n",
       "      <td>0.240</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.8990</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.805</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.913</td>\n",
       "      <td>1.72</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.0912</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>1.360</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.5190</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.22400</td>\n",
       "      <td>-0.1800</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.84</td>\n",
       "      <td>-0.6550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>0.986</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-3.22</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-0.0934</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.644</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.1350</td>\n",
       "      <td>-0.1770</td>\n",
       "      <td>-1.30000</td>\n",
       "      <td>-0.2520</td>\n",
       "      <td>0.229</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>1.190</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.0497</td>\n",
       "      <td>-0.1990</td>\n",
       "      <td>-0.75500</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.0469</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X56    X59   X67    X83   X96    X98   X132   X133    X136    X145  \\\n",
       "0  0.150 -0.841 -2.33  1.360 -1.78 -0.984 -0.362 -0.198 -0.5190 -0.0174   \n",
       "1  0.491 -0.891 -2.30  1.220 -1.74 -0.497 -0.578  0.268 -0.5570  0.6600   \n",
       "2  0.150 -0.841 -2.33  1.360 -1.78 -0.984 -0.362 -0.198 -0.5190 -0.0174   \n",
       "3 -0.106  0.644 -2.14  0.974 -1.20 -1.380  0.540  0.142 -0.1350 -0.1770   \n",
       "4 -0.116 -0.149 -2.43  1.190 -1.38 -1.220  0.121  0.166 -0.0497 -0.1990   \n",
       "\n",
       "      X147    X148   X149  X152    X157   X160   X171   X176   X177  X188  \\\n",
       "0 -0.22400 -0.1800 -0.445  1.84 -0.6550 -1.620 -0.793  1.170  0.687 -1.88   \n",
       "1 -0.00985 -0.1180  0.240  1.35 -0.8990 -1.350 -0.559  0.708  0.805 -1.73   \n",
       "2 -0.22400 -0.1800 -0.445  1.84 -0.6550 -1.620 -0.793  1.170  0.687 -1.88   \n",
       "3 -1.30000 -0.2520  0.229  1.21  0.6610 -0.834 -1.040  1.370  0.421 -1.32   \n",
       "4 -0.75500  0.0363 -0.270  1.58 -0.0469 -0.989 -1.090  0.956  0.210 -1.40   \n",
       "\n",
       "    X195   X197  X201  X202   X204   X208  X213  X218   X228  X234    X239  \\\n",
       "0  0.694 -0.387 -1.45  1.30 -1.620  0.986  1.58 -3.22 -0.717  2.33 -0.0934   \n",
       "1  0.392 -0.277 -1.20  1.34 -1.500  0.913  1.72 -2.91 -0.646  2.00 -0.0912   \n",
       "2  0.694 -0.387 -1.45  1.30 -1.620  0.986  1.58 -3.22 -0.717  2.33 -0.0934   \n",
       "3 -0.629  0.405 -1.21  1.16 -0.771  0.341  0.76 -3.54 -0.372  2.01  1.5500   \n",
       "4  0.187 -0.172 -1.11  1.31 -1.150  0.964  1.96 -3.19 -0.763  2.41  0.6050   \n",
       "\n",
       "    X242   X245     y  \n",
       "0 -1.420  0.260  0.57  \n",
       "1 -0.784  0.233  0.20  \n",
       "2 -1.420  0.260  0.84  \n",
       "3 -1.560  0.239  0.80  \n",
       "4 -1.440  0.323  0.82  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data into a dataframe \n",
    "dataset = pd.read_csv('C:/Users/Administrator/Desktop/机器学习/H1.csv')\n",
    "\n",
    "# Display top of dataframe\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9f0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0466abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(564, 34)\n",
      "(141, 34)\n",
      "(564,)\n",
      "(141,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)  # (404, 13)\n",
    "print(X_test.shape)  # (102, 13)\n",
    "print(y_train.shape)  # (404, )\n",
    "print(y_test.shape)  # (102, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c110c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te =pd.Series(y_test)\n",
    "y_te.to_csv('C:/Users/Administrator/Desktop/机器学习/RNN/y_te15.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06cd512",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4466d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(X_train)\n",
    "x_test = np.array(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_ = torch.Tensor(x_train)\n",
    "y_train_ = torch.Tensor(y_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(x_train_, y_train_)\n",
    "data_iter_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "\n",
    "x_test_ = torch.Tensor(x_test)\n",
    "x_test_1 = x_test_.unsqueeze(1)  # 扩充维度\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.code = nn.Sequential(\n",
    "            # 输出通道数==卷积核个数, 卷积->BN->激活->池化\n",
    "            nn.Conv1d(in_channels=1, out_channels=10, kernel_size=3, stride=1),  # (1,13)--> (10,11)\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),  # (10,11)-->(10,9)\n",
    "\n",
    "            nn.Conv1d(in_channels=10, out_channels=20, kernel_size=3, stride=1),  # (10,9)-->(20,7)\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),  # (20,7)-->(20,5)\n",
    "\n",
    "            # 展平\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=520, out_features=50),\n",
    "            nn.Linear(in_features=50, out_features=25),\n",
    "            nn.Linear(in_features=25, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.code(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 模型实例化\n",
    "net = ConvNet()\n",
    "# 损失函数\n",
    "criterion = nn.MSELoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 500\n",
    "loss_train = []\n",
    "for epoch in range(epochs):\n",
    "    # 训练模型 / train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(data_iter_train):\n",
    "        input_data = data[0]\n",
    "        # print(\"1\",input_data.shape)\n",
    "        input_data = input_data.unsqueeze(1)  # 扩充维度\n",
    "        # print(\"2\",input_data.shape)\n",
    "        label = data[1]\n",
    "        label = label.unsqueeze(1)  # 扩充维度\n",
    "        # print(\"label shape\",label.shape)\n",
    "        output_data = net(input_data)\n",
    "        # print(output_data)\n",
    "        # print(output_data.shape)\n",
    "        loss = criterion(output_data, label)\n",
    "        # print(\"loss:\",loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    print(running_loss / len(data_iter_train))\n",
    "    loss_train.append(running_loss / len(data_iter_train))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('train loss')  # 图片名称\n",
    "plt.xlabel(\"epoch\")  # 横坐标名称\n",
    "plt.ylabel(\"loss\")  # 纵坐标名称\n",
    "plt.plot(np.arange(len(loss_train)), loss_train, label='train')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('loss.png', dpi=600)  # 保存图片\n",
    "plt.show()\n",
    "\n",
    "output_test = net(x_test_1)\n",
    "y_predict = output_test.detach().numpy().reshape(-1)\n",
    "acc_test = [((np.abs(y_predict - y_test) <= 10.).sum() / x_test_.shape[0]) * 100]\n",
    "print(\"acc test:\",acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f468fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sn\n",
    "sn.regplot(x=y_predict, y=y_test,line_kws={\"lw\":2,'ls':'--','color':'black',\"alpha\":0.7})\n",
    "plt.xlabel('Predicted yeild', color='blue')\n",
    "plt.ylabel('Measured yeild', color ='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\n",
    "# R^2 (coefficient of determination) regression score function: \n",
    "R2_mr =r2_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0122703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R2_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict2=pd.Series(y_predict)\n",
    "\n",
    "y_predict2.to_csv('C:/Users/Administrator/Desktop/y_predict128csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e13f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data into a dataframe \n",
    "P_x = pd.read_csv('C:/Users/Administrator/Desktop/tj1.csv')\n",
    "\n",
    "p_x = np.array(P_x)\n",
    "\n",
    "p_x_ = torch.Tensor(p_x)\n",
    "p_x_1 = p_x_.unsqueeze(1)  # 扩充维度\n",
    "\n",
    "p_test = net(p_x_1)\n",
    "\n",
    "y_pre_ = p_test.detach().numpy().reshape(-1)\n",
    "\n",
    "y_predict3=pd.Series(y_pre_)\n",
    "\n",
    "y_predict3.to_csv('C:/Users/Administrator/Desktop/y_predict119.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d00a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde30dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86473783",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-556d9c414041>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    cmd /c for /f %I in ('wmic process get Name')do (wmic process where Name=\"%I\" delete)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cmd /c for /f %I in ('wmic process get Name')do (wmic process where Name=\"%I\" delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7467f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
